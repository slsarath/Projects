# ============================================
# Feature Engineering for Call Transcripts
# ============================================
import re
import gc
import numpy as np
import pandas as pd

from sklearn.preprocessing import MinMaxScaler, OneHotEncoder, MultiLabelBinarizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.utils.validation import check_is_fitted
from scipy import sparse as sp

# Sentence embeddings
from sentence_transformers import SentenceTransformer

# HuggingFace sentiment (twitter-roberta)
from transformers import pipeline


# -----------------------------
# Config
# -----------------------------
RANDOM_SEED = 42
PRIMARY_TEXT_COL = "customer_light"   # main column for text features
FALLBACK_TEXT_COL = "full_light"      # if primary is empty
NEG_SCORE_COL = "Max negative customer score"
COST_COL = "Total cost"               # optional; included if present
MATERIALITY_COL = "Materiality"       # expected: "COMPLAINT", "CONCERN", or null

# Treat 'sep' as a turn delimiter (since [SEP] became sep in your pipeline)
TURN_TOKEN = r"\bsep\b"

# Keyword map (tweak as needed)
CATEGORY_KEYWORDS = {
    "Refund": ["refund", "money back", "reimbursement", "return my money"],
    "Disconnection": ["disconnect", "cut off", "call drop", "line cut"],
    "Complaint": ["complaint", "issue", "problem", "escalate", "dissatisfied", "disgraceful", "terrible", "awful"],
    "Concern": ["concern", "worry", "not happy", "doubt", "hesitation", "unsure", "confused"],
    "Payment Issue": ["payment failed", "transaction declined", "billing error", "charged twice"],
    "Product Issue": ["defective", "not working", "broken", "faulty", "malfunction"],
    "Service Delay": ["delay", "waiting", "late service", "not delivered"],
    "Technical Issue": ["technical error", "system down", "bug", "glitch", "app crash", "login"],
    "Account Access": ["login failed", "password reset", "account locked", "access denied"],
    "Fraud/Security": ["fraud", "unauthorized", "hacked", "phishing", "scam"],
    "Cancellation": ["cancel subscription", "terminate", "close account"],
    "Upgrade Request": ["upgrade plan", "better offer", "higher package"],
    "General Query": ["information", "clarify", "know more", "details"],
    "Feedback": ["feedback", "suggestion", "recommendation"],
    "Positive Mention": ["happy", "good service", "satisfied", "thank you"],
}

# -----------------------------
# Utilities
# -----------------------------
def choose_text_series(df: pd.DataFrame) -> pd.Series:
    """Prefer PRIMARY_TEXT_COL; if empty/missing fallback to FALLBACK_TEXT_COL."""
    if PRIMARY_TEXT_COL in df.columns:
        s = df[PRIMARY_TEXT_COL].fillna("").astype(str)
        if s.str.strip().str.len().sum() > 0:
            return s
    if FALLBACK_TEXT_COL in df.columns:
        return df[FALLBACK_TEXT_COL].fillna("").astype(str)
    raise KeyError(f"Neither '{PRIMARY_TEXT_COL}' nor '{FALLBACK_TEXT_COL}' present.")

def chunk_text(text: str, max_chars: int = 1000):
    if not isinstance(text, str) or not text:
        return []
    return [text[i:i+max_chars] for i in range(0, len(text), max_chars)]

def encode_with_chunks(model, text: str):
    """Mean-pool embeddings over chunks."""
    if not isinstance(text, str) or text.strip() == "":
        return np.zeros(model.get_sentence_embedding_dimension(), dtype=np.float32)
    chunks = chunk_text(text, max_chars=1000)
    vecs = model.encode(chunks, convert_to_tensor=True, truncate=True)
    return vecs.mean(dim=0).cpu().numpy()

def aggregate_sentiment(pipe, text: str):
    """Run twitter-roberta on chunks and average label scores."""
    if not isinstance(text, str) or text.strip() == "":
        return {"roberta_label": None, "roberta_score": np.nan,
                "roberta_neg": np.nan, "roberta_neu": np.nan, "roberta_pos": np.nan}
    chunks = chunk_text(text, max_chars=512)
    buckets = {"NEGATIVE": [], "NEUTRAL": [], "POSITIVE": []}
    for ch in chunks:
        try:
            out = pipe(ch[:512])[0]
            buckets[out["label"].upper()].append(out["score"])
        except Exception:
            continue
    if not any(len(v) for v in buckets.values()):
        return {"roberta_label": None, "roberta_score": np.nan,
                "roberta_neg": np.nan, "roberta_neu": np.nan, "roberta_pos": np.nan}
    avg = {k: float(np.mean(v)) if v else 0.0 for k, v in buckets.items()}
    best_label = max(avg, key=avg.get)
    return {"roberta_label": best_label, "roberta_score": avg[best_label],
            "roberta_neg": avg["NEGATIVE"], "roberta_neu": avg["NEUTRAL"], "roberta_pos": avg["POSITIVE"]}

def count_occurrences(pattern: str, text: str) -> int:
    if not isinstance(text, str):
        return 0
    return len(re.findall(pattern, text))

def count_money_mentions(text: str) -> int:
    if not isinstance(text, str):
        return 0
    # robust: [MONEY], [money], or bare token "money"
    return len(re.findall(r"\[?money\]?", text, flags=re.IGNORECASE))

def tag_transcript_keywords(text: str) -> list:
    if not isinstance(text, str) or not text:
        return []
    txt = text.lower()
    tags = []
    for cat, kws in CATEGORY_KEYWORDS.items():
        if any(kw in txt for kw in kws):
            tags.append(cat)
    return tags

def structural_features(df: pd.DataFrame) -> pd.DataFrame:
    """Create simple, cheap structure/lexical features from cleaned columns."""
    cust = df.get("customer_light", pd.Series([""] * len(df)))
    full_ = df.get("full_light", pd.Series([""] * len(df)))

    f = pd.DataFrame(index=df.index)
    # token counts
    f["cust_tokens"] = cust.fillna("").str.split().apply(len)
    f["full_tokens"] = full_.fillna("").str.split().apply(len)
    f["type_token_ratio"] = (cust.fillna("").str.split().apply(lambda x: len(set(x)) / (len(x) + 1e-6)))
    # turns via 'sep'
    f["cust_turns"] = cust.fillna("").apply(lambda x: count_occurrences(TURN_TOKEN, x) + 1 if x.strip() else 0)
    f["full_turns"] = full_.fillna("").apply(lambda x: count_occurrences(TURN_TOKEN, x) + 1 if x.strip() else 0)
    # avg turn length (tokens)
    f["cust_avg_turn_len"] = f.apply(lambda r: r["cust_tokens"] / max(r["cust_turns"], 1), axis=1)
    f["full_avg_turn_len"] = f.apply(lambda r: r["full_tokens"] / max(r["full_turns"], 1), axis=1)
    # punctuation proxies (on full_light which keeps ?, !)
    f["qmarks"] = full_.fillna("").str.count(r"\?")
    f["exclaims"] = full_.fillna("").str.count(r"\!")
    # money mentions
    f["money_mentions"] = cust.fillna("").apply(count_money_mentions)
    # basic lexicon flags
    f["kw_complaint_hits"] = cust.fillna("").str.count(r"\bcomplain\w*|\bdissatisf\w*|\bissue\b")
    f["kw_concern_hits"] = cust.fillna("").str.count(r"\bconcern\w*|\bworr\w*|\bnot happy\b")
    f["kw_fraud_hits"] = cust.fillna("").str.count(r"\bfraud\w*|\bunauthori\w*|\bscam\w*|\bhack\w*")
    return f

def build_targets(df: pd.DataFrame):
    """Returns y_binary (Complaint vs Concern) and y_multiclass (Neutral/Concern/Complaint)."""
    mat = df.get(MATERIALITY_COL)
    if mat is None:
        raise KeyError(f"'{MATERIALITY_COL}' column not found.")
    mat = mat.fillna("NEUTRAL").str.upper().str.strip()
    # 3-class
    map3 = {"NEUTRAL": 0, "CONCERN": 1, "COMPLAINT": 2}
    y_multiclass = mat.map(map3).astype("Int64")
    # Binary (drop NEUTRAL)
    mask_bc = mat.isin(["COMPLAINT", "CONCERN"])
    y_binary = mat[mask_bc].map({"CONCERN": 0, "COMPLAINT": 1}).astype("Int64")
    return y_binary, mask_bc, y_multiclass

# -----------------------------
# Main FE function
# -----------------------------
def engineer_features(df_clean: pd.DataFrame,
                      compute_roberta: bool = True,
                      compute_tfidf: bool = True,
                      compute_embeddings: bool = True,
                      tfidf_max_features: int = 20000,
                      tfidf_ngram: tuple = (1, 2),
                      device: str = None):
    """
    Returns:
      features = {
        'X_sparse': <csr_matrix>  (TF-IDF + meta),
        'X_dense':  <ndarray>     (MPNet + meta),
        'meta_df':  <DataFrame>   (numeric + encoded),
        'tfidf':    <Vectorizer or None>,
        'emb_model':<SentenceTransformer or None>,
        'ohe_categories': <list>,
        'mlb_classes': <list>,
        'y_binary': <Series>,
        'mask_binary': <Series[bool]>,
        'y_multiclass': <Series>
      }
    """
    df = df_clean.copy()

    # Choose text column
    text_series = choose_text_series(df)

    # ---------------------------
    # 0) Keyword tags (multi-hot)
    # ---------------------------
    if "keyword_tags" not in df.columns:
        df["keyword_tags"] = text_series.apply(tag_transcript_keywords)
    mlb = MultiLabelBinarizer()
    tags_mlb = mlb.fit_transform(df["keyword_tags"].apply(lambda x: x if isinstance(x, list) else []))
    tags_df = pd.DataFrame(tags_mlb, index=df.index, columns=[f"tag_{c}" for c in mlb.classes_])

    # ---------------------------
    # 1) Sentiment (twitter-roberta)
    # ---------------------------
    if compute_roberta:
        sentiment_pipe = pipeline(
            "sentiment-analysis",
            model="cardiffnlp/twitter-roberta-base-sentiment",
            truncation=True,
            device=0 if device == "cuda" else -1
        )
        sent_rows = [aggregate_sentiment(sentiment_pipe, t) for t in text_series.tolist()]
        sent_df = pd.DataFrame(sent_rows, index=df.index)
        df = pd.concat([df, sent_df], axis=1)
    else:
        # ensure columns exist
        for col in ["roberta_label", "roberta_score", "roberta_neg", "roberta_neu", "roberta_pos"]:
            if col not in df.columns:
                df[col] = np.nan

    # one-hot roberta_label
    ohe = OneHotEncoder(sparse=False, handle_unknown="ignore")
    lbl = df[["roberta_label"]].fillna("NEUTRAL")
    lbl_ohe = ohe.fit_transform(lbl)
    sentiment_ohe_df = pd.DataFrame(lbl_ohe, index=df.index,
                                    columns=[f"sent_{c}" for c in ohe.categories_[0]])

    # ---------------------------
    # 2) Structural & numeric features
    # ---------------------------
    struct_df = structural_features(df)

    # numeric: negative score (+ cost if present)
    numeric_cols = []
    if NEG_SCORE_COL in df.columns:
        numeric_cols.append(NEG_SCORE_COL)
    if COST_COL in df.columns:
        # Coerce numerics safely
        df[COST_COL] = pd.to_numeric(
            df[COST_COL].astype(str).str.replace(r"[^\d.]", "", regex=True),
            errors="coerce"
        )
        numeric_cols.append(COST_COL)

    numeric_df = df[numeric_cols].copy() if numeric_cols else pd.DataFrame(index=df.index)
    # add roberta scalar scores
    for col in ["roberta_score", "roberta_neg", "roberta_neu", "roberta_pos"]:
        if col in df.columns:
            numeric_df[col] = df[col]

    # scale numerics
    if numeric_df.shape[1] > 0:
        scaler = MinMaxScaler()
        numeric_df[:] = scaler.fit_transform(numeric_df.fillna(0.0))
    else:
        scaler = None

    # ---------------------------
    # 3) Meta feature block
    # ---------------------------
    meta_df = pd.concat([numeric_df, sentiment_ohe_df, tags_df, struct_df], axis=1).fillna(0.0)

    # ---------------------------
    # 4) TF-IDF (sparse)
    # ---------------------------
    if compute_tfidf:
        tfidf = TfidfVectorizer(
            max_features=tfidf_max_features,
            ngram_range=tfidf_ngram,
            token_pattern=r"(?u)\b\w+\b"  # keep 'sep' as a token
        )
        X_tfidf = tfidf.fit_transform(text_series.fillna(""))
        X_sparse = sp.hstack([X_tfidf, sp.csr_matrix(meta_df.values)], format="csr")
    else:
        tfidf, X_sparse = None, None

    # ---------------------------
    # 5) MPNet embeddings (dense)
    # ---------------------------
    if compute_embeddings:
        emb_model = SentenceTransformer("sentence-transformers/all-mpnet-base-v2", device=device or "cpu")
        emb_dim = emb_model.get_sentence_embedding_dimension()
        embs = np.vstack([encode_with_chunks(emb_model, t) for t in text_series.tolist()]).astype(np.float32)
        X_dense = np.hstack([embs, meta_df.values.astype(np.float32)])
    else:
        emb_model, X_dense = None, None

    # ---------------------------
    # 6) Targets
    # ---------------------------
    y_binary, mask_binary, y_multiclass = build_targets(df)

    # Housekeeping
    gc.collect()

    return {
        "X_sparse": X_sparse,
        "X_dense": X_dense,
        "meta_df": meta_df,
        "tfidf": tfidf,
        "emb_model": emb_model,
        "ohe_categories": list(ohe.categories_[0]),
        "mlb_classes": list(mlb.classes_),
        "y_binary": y_binary,            # Complaint(1) vs Concern(0) (neutral dropped via mask)
        "mask_binary": mask_binary,      # use this mask to subset X_* and y_binary
        "y_multiclass": y_multiclass,    # 0=Neutral, 1=Concern, 2=Complaint
    }


# =========================================================
# Example usage
# =========================================================
if __name__ == "__main__":
    # Load your already preprocessed dataframe (with full_light / customer_light / etc.)
    df_clean = pd.read_csv("transcripts_clean.csv")  # <- replace with your path

    feats = engineer_features(
        df_clean,
        compute_roberta=True,          # uses twitter-roberta-base-sentiment
        compute_tfidf=True,
        compute_embeddings=True,       # uses all-mpnet-base-v2
        tfidf_max_features=20000,
        tfidf_ngram=(1, 2),
        device=None                    # "cuda" if you have GPU; else None/CPU
    )

    # Sparse (TF-IDF + meta)
    Xs = feats["X_sparse"]
    # Dense (MPNet + meta)
    Xd = feats["X_dense"]

    # Targets
    y_bin = feats["y_binary"]
    m_bin = feats["mask_binary"]
    y_3c = feats["y_multiclass"]

    print("Sparse shape:", None if Xs is None else Xs.shape)
    print("Dense shape:", None if Xd is None else Xd.shape)
    print("Meta cols:", feats["meta_df"].shape[1])
    print("OHE sentiment cats:", feats["ohe_categories"])
    print("Keyword tags:", feats["mlb_classes"])

    # Example: train binary classifier on Complaint vs Concern using dense features
    # Only keep rows where binary labels exist (drop NEUTRAL)
    if Xd is not None:
        Xd_bin = Xd[m_bin.values]
        y_bin2 = y_bin.dropna().values.astype(int)

        from sklearn.model_selection import train_test_split
        from xgboost import XGBClassifier
        from sklearn.metrics import classification_report

        Xtr, Xte, ytr, yte = train_test_split(
            Xd_bin, y_bin2, test_size=0.2, random_state=RANDOM_SEED, stratify=y_bin2
        )
        clf = XGBClassifier(
            n_estimators=400, max_depth=6, learning_rate=0.05, subsample=0.9, colsample_bytree=0.9,
            reg_lambda=1.0, eval_metric="logloss", random_state=RANDOM_SEED, n_jobs=-1
        )
        clf.fit(Xtr, ytr)
        pred = clf.predict(Xte)
        print(classification_report(yte, pred, target_names=["CONCERN","COMPLAINT"]))
