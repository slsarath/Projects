import pandas as pd
import numpy as np

# Text vectorization
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import MinMaxScaler

# Sentence embeddings
from sentence_transformers import SentenceTransformer
import torch

# HuggingFace Transformers (RoBERTa Twitter sentiment)
from transformers import pipeline

# ---------------------------------------------------
# 1. Load your dataset
# ---------------------------------------------------
df = pd.read_csv("transcripts.csv")

TEXT_COL = "clean_customer_only"
SENTIMENT_SCORE_COL = "Max negative customer score"
COST_COL = "Total cost"

# ---------------------------------------------------
# 2. Normalize numeric features
# ---------------------------------------------------
df[COST_COL] = pd.to_numeric(
    df[COST_COL].astype(str).str.replace(r"[^\d.]", "", regex=True),
    errors="coerce"
)

scaler = MinMaxScaler()
df[[SENTIMENT_SCORE_COL, COST_COL]] = scaler.fit_transform(
    df[[SENTIMENT_SCORE_COL, COST_COL]]
)

# ---------------------------------------------------
# 3. Text features - TFIDF
# ---------------------------------------------------
tfidf = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))
X_tfidf = tfidf.fit_transform(df[TEXT_COL].fillna(""))

# ---------------------------------------------------
# 4. Sentence Embeddings (with chunking + mean pooling)
# ---------------------------------------------------
models = {
    "mpnet": SentenceTransformer("sentence-transformers/all-mpnet-base-v2"),
    "minilm": SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2"),
    "distilbert": SentenceTransformer("sentence-transformers/all-distilroberta-v1")
}

def chunk_text(text, max_chars=1000):
    """Split long text into chunks (approx â‰ˆ 512 tokens)."""
    return [text[i:i+max_chars] for i in range(0, len(text), max_chars)]

def encode_with_chunks(model, text):
    """Encode text in chunks and mean-pool embeddings."""
    if not isinstance(text, str) or text.strip() == "":
        return np.zeros(model.get_sentence_embedding_dimension())
    chunks = chunk_text(text)
    vecs = model.encode(chunks, convert_to_tensor=True, truncate=True)
    return vecs.mean(dim=0).cpu().numpy()

valid_texts = df[TEXT_COL].fillna("").tolist()
embeddings = {}

for name, model in models.items():
    print(f"Encoding with {name}...")
    embeddings[name] = np.vstack([encode_with_chunks(model, t) for t in valid_texts])
    print(f"{name} embeddings shape:", embeddings[name].shape)

# ---------------------------------------------------
# 5. RoBERTa Twitter sentiment (with chunking + mean pooling)
# ---------------------------------------------------
sentiment_pipe = pipeline("sentiment-analysis",
                          model="cardiffnlp/twitter-roberta-base-sentiment",
                          truncation=True)

def aggregate_sentiment(text):
    """Run sentiment on chunks and average scores across chunks."""
    if not isinstance(text, str) or text.strip() == "":
        return {"roberta_label": None, "roberta_score": None}

    chunks = chunk_text(text, max_chars=512)
    scores = {"NEGATIVE": [], "NEUTRAL": [], "POSITIVE": []}

    for ch in chunks:
        try:
            out = sentiment_pipe(ch[:512])[0]  # safe truncate
            scores[out["label"].upper()].append(out["score"])
        except Exception:
            continue

    if not any(scores.values()):
        return {"roberta_label": None, "roberta_score": None}

    avg_scores = {k: np.mean(v) if v else 0.0 for k, v in scores.items()}
    best_label = max(avg_scores, key=avg_scores.get)
    return {"roberta_label": best_label, "roberta_score": avg_scores[best_label]}

roberta_preds = [aggregate_sentiment(t) for t in df[TEXT_COL].fillna("").tolist()]
roberta_df = pd.DataFrame(roberta_preds)
df = pd.concat([df.reset_index(drop=True), roberta_df], axis=1)

# ---------------------------------------------------
# 6. Semantic keyword-based tagging
# ---------------------------------------------------
category_keywords = {
    "Refund": ["refund", "money back", "reimbursement", "return my money"],
    "Disconnection": ["disconnect", "cut off", "call drop", "line cut"],
    "Complaint": ["complaint", "issue", "problem", "escalate", "dissatisfied"],
    "Concern": ["concern", "worry", "not happy", "doubt", "hesitation"],
    "Payment Issue": ["payment failed", "transaction declined", "billing error", "charged twice"],
    "Product Issue": ["defective", "not working", "broken", "faulty", "malfunction"],
    "Service Delay": ["delay", "waiting", "late service", "not delivered"],
    "Technical Issue": ["technical error", "system down", "bug", "glitch"],
    "Account Access": ["login failed", "password reset", "account locked"],
    "Fraud/Security": ["fraud", "unauthorized", "hacked", "phishing", "scam"],
    "Cancellation": ["cancel subscription", "terminate", "close account"],
    "Upgrade Request": ["upgrade plan", "better offer", "higher package"],
    "General Query": ["information", "clarify", "know more", "details"],
    "Feedback": ["feedback", "suggestion", "recommendation"],
    "Positive Mention": ["happy", "good service", "satisfied", "thank you"],
}

def tag_transcript(text):
    tags = []
    txt = text.lower()
    for category, kws in category_keywords.items():
        if any(kw in txt for kw in kws):
            tags.append(category)
    return tags if tags else None

df["keyword_tags"] = df[TEXT_COL].fillna("").apply(tag_transcript)

# ---------------------------------------------------
# Final outputs
# ---------------------------------------------------
print("TF-IDF shape:", X_tfidf.shape)
for k, v in embeddings.items():
    print(f"{k} embeddings shape:", v.shape)

print(df.head())