#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
End-to-end feature engineering + model training + save/load + inference
for your transcript dataset (columns matched to your screenshot).
"""

import os
import re
import gc
import joblib
import pickle
import numpy as np
import pandas as pd
from typing import Optional, Tuple, Dict, Any, List

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import MinMaxScaler, MultiLabelBinarizer
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from sklearn.linear_model import LogisticRegression
from scipy import sparse as sp

# Sentence embeddings
from sentence_transformers import SentenceTransformer

# Optional sentiment (HuggingFace)
from transformers import pipeline

# -----------------------------
# Config / Column names
# -----------------------------
INPUT_CSV = "input_transcripts.csv"         # replace with your file
OUTPUT_DIR = "artifacts"
os.makedirs(OUTPUT_DIR, exist_ok=True)

COL_CALL = "call_transcript"
COL_CUSTOMER_LIGHT = "customer_light"
COL_FULL_LIGHT = "full_light"
COL_MATERIALITY = "materiality"             # values: CONCERN, COMPLAINT, or null
COL_LOUDNESS = "loudnessscore"
COL_MAX_NEG = "Max negative customer score" # exact from screenshot (case sensitive)
# fallback names if people differ
ALT_MAX_NEG = ["Max Negative Sentiment Score", "Max negative sentiment score"]

SEP_TOKEN_REGEX = r"\bsep\b"               # your pipeline turned [SEP] into 'sep'
EMBED_MODEL_NAME = "sentence-transformers/all-mpnet-base-v2"
TFIDF_MAX_FEAT = 20000
TFIDF_NGRAM = (1, 2)
RANDOM_SEED = 42

# Keyword dictionary (extend if needed)
CATEGORY_KEYWORDS = {
    "Refund": ["refund", "money back", "reimbursement", "return my money"],
    "Disconnection": ["disconnect", "cut off", "call drop", "line cut"],
    "Complaint": ["complaint", "issue", "problem", "escalate", "dissatisfied", "disgraceful"],
    "Concern": ["concern", "worry", "not happy", "doubt", "hesitation"],
    "Payment Issue": ["payment failed", "transaction declined", "billing error", "charged twice"],
    "Technical Issue": ["technical error", "system down", "bug", "glitch", "login"],
    "Fraud/Security": ["fraud", "unauthorized", "hacked", "phishing", "scam"],
    "General Query": ["information", "clarify", "know more", "details"],
    "Positive Mention": ["happy", "good service", "satisfied", "thank you"],
}

# -----------------------------
# Helpers
# -----------------------------
def pick_max_neg_col(df: pd.DataFrame) -> Optional[str]:
    if COL_MAX_NEG in df.columns:
        return COL_MAX_NEG
    for alt in ALT_MAX_NEG:
        if alt in df.columns:
            return alt
    return None

def choose_text_series(df: pd.DataFrame) -> pd.Series:
    """Prefer customer_light; fallback to full_light or call_transcript."""
    for c in (COL_CUSTOMER_LIGHT, COL_FULL_LIGHT, COL_CALL):
        if c in df.columns:
            s = df[c].fillna("").astype(str)
            if s.str.strip().str.len().sum() > 0:
                return s
    raise KeyError("No suitable text column found (need customer_light/full_light/call_transcript).")

def chunk_text(text: str, max_chars: int = 1000) -> List[str]:
    if not isinstance(text, str) or not text.strip():
        return []
    return [text[i:i+max_chars] for i in range(0, len(text), max_chars)]

def count_occurrences(pattern: str, text: str) -> int:
    if not isinstance(text, str) or not text:
        return 0
    return len(re.findall(pattern, text))

def count_money_mentions(text: str) -> int:
    if not isinstance(text, str) or not text:
        return 0
    return len(re.findall(r"\[?money\]?", text, flags=re.IGNORECASE))

def tag_keywords(text: str) -> List[str]:
    if not isinstance(text, str) or not text:
        return []
    txt = text.lower()
    tags = []
    for k, kws in CATEGORY_KEYWORDS.items():
        if any(kw in txt for kw in kws):
            tags.append(k)
    return tags

# -----------------------------
# Feature engineering class
# -----------------------------
class FEEngine:
    def __init__(self,
                 tfidf_max_features: int = TFIDF_MAX_FEAT,
                 tfidf_ngram: Tuple[int, int] = TFIDF_NGRAM,
                 embed_model_name: str = EMBED_MODEL_NAME,
                 compute_sentiment_auto: bool = True,
                 device: Optional[str] = None):
        self.tfidf_max_features = tfidf_max_features
        self.tfidf_ngram = tfidf_ngram
        self.embed_model_name = embed_model_name
        self.compute_sentiment_auto = compute_sentiment_auto
        self.device = device if device else ("cuda" if (torch := __import__("torch")).cuda.is_available() else "cpu")
        self.tfidf = None
        self.mlb = None
        self.scaler = None
        self.emb_model = None
        self.sent_pipe = None
        self._fitted = False

    # structural features
    def _structural(self, df: pd.DataFrame) -> pd.DataFrame:
        cust = df.get(COL_CUSTOMER_LIGHT, pd.Series([""] * len(df))).fillna("").astype(str)
        full = df.get(COL_FULL_LIGHT, pd.Series([""] * len(df))).fillna("").astype(str)
        out = pd.DataFrame(index=df.index)
        out["cust_tokens"] = cust.str.split().apply(lambda x: len(x) if isinstance(x, list) else 0)
        out["full_tokens"] = full.str.split().apply(lambda x: len(x) if isinstance(x, list) else 0)
        out["type_token_ratio"] = cust.str.split().apply(lambda x: len(set(x)) / (len(x) + 1e-6) if isinstance(x, list) and x else 0.0)
        out["cust_turns"] = cust.apply(lambda x: count_occurrences(SEP_TOKEN_REGEX, x) + 1 if x.strip() else 0)
        out["full_turns"] = full.apply(lambda x: count_occurrences(SEP_TOKEN_REGEX, x) + 1 if x.strip() else 0)
        out["cust_avg_turn_len"] = out.apply(lambda r: r["cust_tokens"] / max(r["cust_turns"], 1), axis=1)
        out["full_avg_turn_len"] = out.apply(lambda r: r["full_tokens"] / max(r["full_turns"], 1), axis=1)
        out["qmarks"] = full.str.count(r"\?")
        out["exclaims"] = full.str.count(r"\!")
        out["money_mentions"] = cust.apply(count_money_mentions)
        out["kw_complaint_hits"] = cust.str.count(r"\bcomplain\w*|\bdissatisf\w*|\bissue\b")
        out["kw_concern_hits"] = cust.str.count(r"\bconcern\w*|\bworr\w*|\bnot happy\b")
        out["kw_fraud_hits"] = cust.str.count(r"\bfraud\w*|\bunauthori\w*|\bscam\w*|\bhack\w*")
        return out.fillna(0.0)

    # numeric block (including Max negative score + loudnessscore)
    def _numeric(self, df: pd.DataFrame) -> pd.DataFrame:
        cols = {}
        # find max neg column
        maxneg = pick_max_neg_col(df)
        if maxneg:
            cols["max_neg"] = pd.to_numeric(df[maxneg].astype(str).str.replace(r"[^\d\-\.\+eE]", "", regex=True), errors="coerce").fillna(0.0)
        else:
            cols["max_neg"] = pd.Series(0.0, index=df.index)
        # loudness if present
        if COL_LOUDNESS in df.columns:
            cols["loudnessscore"] = pd.to_numeric(df[COL_LOUDNESS], errors="coerce").fillna(0.0)
        else:
            cols["loudnessscore"] = pd.Series(0.0, index=df.index)
        # any Total cost-like columns - try to pick common variants
        for c in ["Total cost", "Total Cost", "TotalCost"]:
            if c in df.columns:
                cols["total_cost"] = pd.to_numeric(df[c].astype(str).str.replace(r"[^\d\.]", "", regex=True), errors="coerce").fillna(0.0)
                break
        if "total_cost" not in cols:
            cols["total_cost"] = pd.Series(0.0, index=df.index)
        return pd.DataFrame(cols, index=df.index)

    def _compute_sentiment_probs(self, texts: pd.Series) -> pd.DataFrame:
        if self.sent_pipe is None:
            # load sentiment pipeline
            self.sent_pipe = pipeline(
                "sentiment-analysis",
                model="cardiffnlp/twitter-roberta-base-sentiment",
                truncation=True,
                device=0 if self.device == "cuda" else -1
            )
        rows = []
        for t in texts.tolist():
            if not isinstance(t, str) or not t.strip():
                rows.append({"roberta_neg": 0.0, "roberta_neu": 0.0, "roberta_pos": 0.0})
                continue
            acc = {"NEGATIVE": [], "NEUTRAL": [], "POSITIVE": []}
            chunks = chunk_text(t, max_chars=512)
            for ch in chunks:
                try:
                    out = self.sent_pipe(ch[:512])[0]
                    acc[out["label"].upper()].append(out["score"])
                except Exception:
                    continue
            if not any(len(v) for v in acc.values()):
                rows.append({"roberta_neg": 0.0, "roberta_neu": 0.0, "roberta_pos": 0.0})
            else:
                rows.append({
                    "roberta_neg": float(np.mean(acc["NEGATIVE"])) if acc["NEGATIVE"] else 0.0,
                    "roberta_neu": float(np.mean(acc["NEUTRAL"])) if acc["NEUTRAL"] else 0.0,
                    "roberta_pos": float(np.mean(acc["POSITIVE"])) if acc["POSITIVE"] else 0.0,
                })
        return pd.DataFrame(rows, index=texts.index)

    # fit TF-IDF and basic artifacts
    def fit(self, df: pd.DataFrame):
        texts = choose_text_series(df)
        # TF-IDF
        self.tfidf = TfidfVectorizer(max_features=self.tfidf_max_features, ngram_range=self.tfidf_ngram, token_pattern=r"(?u)\b\w+\b")
        _ = self.tfidf.fit_transform(texts.fillna(""))
        # MultiLabelBinarizer for keywords
        self.mlb = MultiLabelBinarizer()
        _ = self.mlb.fit([tag_keywords(t) for t in texts.fillna("")])
        # scaler - fit on numeric + (no sentiment until transform unless auto triggers)
        # create a temporary meta block to fit scaler
        struct = self._structural(df)
        numeric = self._numeric(df)
        # include sentiment if auto decides
        add_sent = self._should_add_sentiment(df)
        if add_sent:
            sent = self._compute_sentiment_probs(texts)
        else:
            sent = pd.DataFrame({"roberta_neg": 0.0, "roberta_neu": 0.0, "roberta_pos": 0.0}, index=df.index)
        scale_block = pd.concat([numeric, sent], axis=1).fillna(0.0)
        self.scaler = MinMaxScaler()
        _ = self.scaler.fit(scale_block.values)
        # Prepare embedding model (lazy load)
        self.emb_model = SentenceTransformer(self.embed_model_name, device=self.device)
        self._fitted = True
        return self

    def _should_add_sentiment(self, df: pd.DataFrame) -> bool:
        if not self.compute_sentiment_auto:
            return False
        if COL_MATERIALITY in df.columns:
            mat = df[COL_MATERIALITY].astype(str).str.upper().str.strip()
            # enable if null / neutral / empty exist
            return mat.isna().any() or (mat == "NEUTRAL").any() or (mat == "").any()
        return True

    # transform -> builds X_sparse, X_dense, meta_df, targets
    def transform(self, df: pd.DataFrame) -> Dict[str, Any]:
        if not self._fitted:
            raise RuntimeError("fit() first")

        texts = choose_text_series(df)
        # TF-IDF transform
        X_tfidf = self.tfidf.transform(texts.fillna(""))

        # structural and numeric
        struct = self._structural(df)
        numeric = self._numeric(df)

        # sentiment
        add_sent = self._should_add_sentiment(df)
        if add_sent:
            sent = self._compute_sentiment_probs(texts)
        else:
            sent = pd.DataFrame({"roberta_neg": 0.0, "roberta_neu": 0.0, "roberta_pos": 0.0}, index=df.index)

        # scale numeric+sentiment
        scale_block = pd.concat([numeric, sent], axis=1).fillna(0.0)
        scaled_vals = pd.DataFrame(self.scaler.transform(scale_block.values), index=df.index, columns=scale_block.columns)

        # keyword multi-hot
        tags = [tag_keywords(t) for t in texts.fillna("")]
        tag_m = self.mlb.transform(tags)
        tag_df = pd.DataFrame(tag_m, index=df.index, columns=[f"tag_{c}" for c in self.mlb.classes_])

        # meta dataframe
        meta_df = pd.concat([scaled_vals, struct, tag_df], axis=1).fillna(0.0)

        # build sparse matrix: TF-IDF + meta (meta as sparse)
        X_sparse = sp.hstack([X_tfidf, sp.csr_matrix(meta_df.values)], format="csr")

        # embeddings (MPNet) -> dense
        model = self.emb_model or SentenceTransformer(self.embed_model_name, device=self.device)
        # encode in batches with chunk-mean pooling
        embs = []
        for t in texts.tolist():
            # mean pooling over chunks (handled in batch by encoding each chunk)
            chunks = chunk_text(t, max_chars=1000)
            if not chunks:
                embs.append(np.zeros(model.get_sentence_embedding_dimension(), dtype=np.float32))
                continue
            vecs = model.encode(chunks, convert_to_tensor=True, truncate=True)
            emb = vecs.mean(dim=0).cpu().numpy()
            embs.append(emb)
        embs = np.vstack(embs).astype(np.float32)
        X_dense = np.hstack([embs, meta_df.values.astype(np.float32)])

        # targets
        if COL_MATERIALITY in df.columns:
            mat = df[COL_MATERIALITY].fillna("NEUTRAL").astype(str).str.upper().str.strip()
            # multiclass map
            map3 = {"NEUTRAL": 0, "CONCERN": 1, "COMPLAINT": 2}
            y_multiclass = mat.map(map3).astype("Int64")
            mask_bin = mat.isin(["COMPLAINT", "CONCERN"])
            y_binary = mat[mask_bin].map({"CONCERN": 0, "COMPLAINT": 1}).astype("Int64")
        else:
            y_multiclass = pd.Series([pd.NA] * len(df), index=df.index, dtype="Int64")
            mask_bin = pd.Series([False] * len(df), index=df.index)
            y_binary = pd.Series([pd.NA] * len(df), index=df.index, dtype="Int64")

        artifacts = {
            "tfidf": self.tfidf,
            "scaler": self.scaler,
            "mlb": self.mlb,
            "embed_model_name": self.embed_model_name,
            "meta_columns": list(meta_df.columns),
            "dense_embedding_dim": embs.shape[1],
            "sparse_dim": X_sparse.shape[1],
        }

        gc.collect()
        return {
            "X_sparse": X_sparse,
            "X_dense": X_dense,
            "meta_df": meta_df,
            "artifacts": artifacts,
            "y_binary": y_binary,
            "mask_binary": mask_bin,
            "y_multiclass": y_multiclass,
        }

    def fit_transform(self, df: pd.DataFrame) -> Dict[str, Any]:
        return self.fit(df).transform(df)

    # persistence helpers
    def save_artifacts(self, out_dir: str = OUTPUT_DIR):
        os.makedirs(out_dir, exist_ok=True)
        joblib.dump(self.tfidf, os.path.join(out_dir, "tfidf.joblib"))
        joblib.dump(self.scaler, os.path.join(out_dir, "scaler.joblib"))
        joblib.dump(self.mlb, os.path.join(out_dir, "mlb.joblib"))
        with open(os.path.join(out_dir, "embed_model.txt"), "w") as f:
            f.write(self.embed_model_name)

    @staticmethod
    def save_matrices(X_sparse, X_dense, meta_df, out_dir: str = OUTPUT_DIR, prefix: str = "fe"):
        os.makedirs(out_dir, exist_ok=True)
        sp.save_npz(os.path.join(out_dir, f"{prefix}_Xsparse.npz"), X_sparse)
        np.save(os.path.join(out_dir, f"{prefix}_Xdense.npy"), X_dense)
        meta_df.to_csv(os.path.join(out_dir, f"{prefix}_meta.csv"), index=False)


# -----------------------------
# Main execution
# -----------------------------
if __name__ == "__main__":
    # Load
    df = pd.read_csv(INPUT_CSV)

    # Instantiate and run FE
    fe = FEEngine(tfidf_max_features=TFIDF_MAX_FEAT, tfidf_ngram=TFIDF_NGRAM, embed_model_name=EMBED_MODEL_NAME, compute_sentiment_auto=True)
    out = fe.fit_transform(df)

    Xs = out["X_sparse"]
    Xd = out["X_dense"]
    meta = out["meta_df"]
    y_bin = out["y_binary"]
    mask_bin = out["mask_binary"]
    y_3c = out["y_multiclass"]

    # Save artifacts + matrices
    fe.save_artifacts(OUTPUT_DIR)
    FEEngine.save_matrices(Xs, Xd, meta, OUTPUT_DIR, prefix="v1")

    # Save embeddings separately (dense part before meta) to speed future reuse:
    # We stored full X_dense (emb + meta) but saving embeddings alone helps reuse.
    emb_dim = fe.emb_model.get_sentence_embedding_dimension()
    embeddings_only = Xd[:, :emb_dim]
    np.save(os.path.join(OUTPUT_DIR, "embeddings.npy"), embeddings_only)

    # Save engineered dataframe (meta appended)
    df_engineered = df.copy()
    df_engineered = pd.concat([df_engineered.reset_index(drop=True), meta.reset_index(drop=True)], axis=1)
    df_engineered.to_csv(os.path.join(OUTPUT_DIR, "transcripts_engineered.csv"), index=False)

    # Train binary classifier (Complaint vs Concern) where labels exist
    if mask_bin.sum() > 10:
        Xd_bin = Xd[mask_bin.values]
        yb = y_bin.dropna().astype(int).values
        Xtr, Xte, ytr, yte = train_test_split(Xd_bin, yb, test_size=0.2, random_state=RANDOM_SEED, stratify=yb)

        # use XGBoost if available else fallback to logistic
        try:
            from xgboost import XGBClassifier
            clf = XGBClassifier(n_estimators=300, max_depth=6, learning_rate=0.05, n_jobs=-1, random_state=RANDOM_SEED, use_label_encoder=False, eval_metric="logloss")
        except Exception:
            clf = LogisticRegression(max_iter=2000, class_weight="balanced", n_jobs=-1)
        clf.fit(Xtr, ytr)
        preds = clf.predict(Xte)
        print("Binary (Complaint vs Concern) classification report:")
        print(classification_report(yte, preds, target_names=["CONCERN", "COMPLAINT"]))

        # persist model
        joblib.dump(clf, os.path.join(OUTPUT_DIR, "complaint_concern_model.joblib"))
    else:
        print("Not enough labeled Complaint/Concern rows to train binary classifier (need >10).")

    # Train multiclass on all rows if labels present
    if y_3c.notna().sum() > 10:
        # use sparse features (TF-IDF + meta) for interpretability + speed
        Xs_full = Xs
        yfull = y_3c.dropna().astype(int).values
        mask_full = y_3c.notna().values
        Xs_full_sub = Xs_full[mask_full]
        Xtr, Xte, ytr, yte = train_test_split(Xs_full_sub, yfull, test_size=0.2, random_state=RANDOM_SEED, stratify=yfull)
        # logistic on sparse
        mclf = LogisticRegression(max_iter=2000, class_weight="balanced")
        mclf.fit(Xtr, ytr)
        preds = mclf.predict(Xte)
        print("Multiclass (Neutral=0, Concern=1, Complaint=2) report:")
        print(classification_report(yte, preds))
        joblib.dump(mclf, os.path.join(OUTPUT_DIR, "multiclass_model.joblib"))
    else:
        print("Not enough multiclass-labeled rows to train multiclass model.")

    print("Artifacts & matrices saved under:", OUTPUT_DIR)

# -----------------------------
# Inference utility (load artifacts + predict)
# -----------------------------
def inference(input_csv: str, artifacts_dir: str = OUTPUT_DIR, output_csv: str = "predictions_output.csv"):
    # Load new data
    df_new = pd.read_csv(input_csv)
    # Load artifacts
    tfidf = joblib.load(os.path.join(artifacts_dir, "tfidf.joblib"))
    scaler = joblib.load(os.path.join(artifacts_dir, "scaler.joblib"))
    mlb = joblib.load(os.path.join(artifacts_dir, "mlb.joblib"))
    # embedding model
    with open(os.path.join(artifacts_dir, "embed_model.txt"), "r") as f:
        emb_name = f.read().strip()
    emb_model = SentenceTransformer(emb_name)
    # load binary model if exists
    model_path = os.path.join(artifacts_dir, "complaint_concern_model.joblib")
    classifier = joblib.load(model_path) if os.path.exists(model_path) else None

    # Build features (minimal reproduce of transform)
    texts = choose_text_series(df_new)
    X_tfidf = tfidf.transform(texts.fillna(""))

    # structural
    fe_local = FEEngine()
    struct = fe_local._structural(df_new)
    numeric = fe_local._numeric(df_new)
    # sentiment disabled in inference for speed; can be enabled if required
    sent = pd.DataFrame({"roberta_neg": 0.0, "roberta_neu": 0.0, "roberta_pos": 0.0}, index=df_new.index)
    scale_block = pd.concat([numeric, sent], axis=1).fillna(0.0)
    scaled = scaler.transform(scale_block.values)
    # tags
    tags = [tag_keywords(t) for t in texts.fillna("")]
    tag_m = mlb.transform(tags)
    tag_df = pd.DataFrame(tag_m, index=df_new.index, columns=[f"tag_{c}" for c in mlb.classes_])
    meta_df = pd.DataFrame(scaled, index=df_new.index, columns=list(scale_block.columns))
    meta_df = pd.concat([meta_df, struct, tag_df], axis=1).fillna(0.0)

    X_sparse_new = sp.hstack([X_tfidf, sp.csr_matrix(meta_df.values)], format="csr")

    # embeddings
    embs = []
    for t in texts.tolist():
        chunks = chunk_text(t, max_chars=1000)
        if not chunks:
            embs.append(np.zeros(emb_model.get_sentence_embedding_dimension(), dtype=np.float32))
            continue
        vecs = emb_model.encode(chunks, convert_to_tensor=True, truncate=True)
        emb = vecs.mean(dim=0).cpu().numpy()
        embs.append(emb)
    embs = np.vstack(embs).astype(np.float32)
    X_dense_new = np.hstack([embs, meta_df.values.astype(np.float32)])

    preds = None
    if classifier is not None:
        # classifier was trained on dense features: load embeddings dim to slice
        clf = classifier
        preds = clf.predict(X_dense_new)
        df_new["pred_binary"] = preds
    else:
        df_new["pred_binary"] = pd.NA

    # Save predictions
    df_new.to_csv(output_csv, index=False)
    return df_new

# end of file
