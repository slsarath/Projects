#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Hybrid 3-class classifier (Complaint, Concern, Neutral) for call transcripts.

Supports CSV and Excel:
- Input auto-detected by extension (.csv, .xlsx, .xls)
- Optional SHEET_NAME for Excel
- Inference output writes .xlsx if output filename ends with .xlsx (otherwise .csv)

Pipeline
--------
1) Build data-driven keyword clusters from TRAIN split (TF-IDF n-grams expanded by embedding similarity).
2) Compute pooled sentence embeddings for each transcript (chunked).
3) Create rule features (loudness, AWS sentiment -5..+5, keyword hits, semantic sims).
4) Train multinomial Logistic Regression on fused features.
5) Blend ML probabilities with rule one-hots for the final decision.
6) Save artifacts + inference function.

Requires: pandas, numpy, scikit-learn, sentence-transformers, openpyxl (for Excel).
"""

# =========================
# Imports & Config
# =========================
import os
import json
import joblib
import numpy as np
import pandas as pd
from typing import List, Tuple, Optional

from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer, ENGLISH_STOP_WORDS
from sklearn.preprocessing import MinMaxScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report

from sentence_transformers import SentenceTransformer

# ----- Paths / Columns -----
INPUT_FILE  = "input_transcripts.xlsx"   # <<< CHANGE HERE (xlsx/xls/csv)
SHEET_NAME  = None                       # <<< CHANGE HERE if your Excel has a specific sheet (e.g., "Sheet1")
OUTPUT_DIR  = "artifacts_hybrid_v1"      # <<< CHANGE HERE
os.makedirs(OUTPUT_DIR, exist_ok=True)

COL_TEXT_CUST   = "customer_light"       # <<< CHANGE HERE if needed
COL_TEXT_FULL   = "full_light"           # optional fallback
COL_LABEL       = "materiality"          # values: NEUTRAL / CONCERN / COMPLAINT (script normalizes)
COL_LOUDNESS    = "loudnessscore"
COL_SENTIMENT   = "Max negative customer score"  # AWS Contact Lens (-5..+5)

# ----- Model / thresholds -----
RANDOM_SEED     = 42
EMBED_MODEL     = "sentence-transformers/all-mpnet-base-v2"  # <<< CHANGE if you prefer
MAX_CHARS_CHUNK = 1000

LOUD_HIGH_TH     = 96.0
SENT_STRONG_NEG  = -3.0
SENT_MOD_NEG_MAX = 0.0
NEU_POS_TH       = 0.5

TFIDF_NGRAM      = (1, 3)
TFIDF_MAX_FEAT   = 30000
TOP_K_CANDIDATES = 4000
TOP_K_PER_CLASS  = 200
SIM_THRESHOLD    = 0.35

ALPHA_RULE_BLEND = 0.55

LABEL2ID = {"NEUTRAL": 0, "CONCERN": 1, "COMPLAINT": 2}
ID2LABEL = {v: k for k, v in LABEL2ID.items()}

# =========================
# IO helpers (CSV/Excel)
# =========================
def read_table(path: str, sheet: Optional[str] = None) -> pd.DataFrame:
    low = path.lower()
    if low.endswith(".csv"):
        return pd.read_csv(path)
    if low.endswith(".xlsx") or low.endswith(".xls"):
        # read; if user passed sheet=None, pick first sheet explicitly to avoid dict return
        if sheet is not None:
            return pd.read_excel(path, sheet_name=sheet, engine="openpyxl")
        # sheet None -> pick the first sheet name
        x = pd.ExcelFile(path, engine="openpyxl")
        first = x.sheet_names[0]
        return pd.read_excel(path, sheet_name=first, engine="openpyxl")
    raise ValueError(f"Unsupported file type: {path}")

def write_table(df: pd.DataFrame, path: str) -> None:
    low = path.lower()
    if low.endswith(".xlsx") or low.endswith(".xls"):
        df.to_excel(path, index=False, engine="openpyxl")
    elif low.endswith(".csv"):
        df.to_csv(path, index=False)
    else:
        df.to_csv(path if "." in path else path + ".csv", index=False)

# =========================
# Minimal text helpers
# =========================
def chunk_text(text: str, max_chars: int = MAX_CHARS_CHUNK) -> List[str]:
    t = (text or "").strip()
    if not t:
        return []
    return [t[i:i+max_chars] for i in range(0, len(t), max_chars)]

def text_series(df: pd.DataFrame) -> pd.Series:
    for c in (COL_TEXT_CUST, COL_TEXT_FULL):
        if c in df.columns:
            s = df[c].fillna("").astype(str)
            if s.str.strip().str.len().sum() > 0:
                return s
    raise KeyError("No suitable text column found (need customer_light or full_light).")

def normalize_labels_inplace(df: pd.DataFrame, label_col: str = COL_LABEL) -> None:
    df[label_col] = (
        df[label_col]
        .fillna("NEUTRAL")
        .astype(str)
        .str.upper()
        .str.strip()
        .replace({
            "COMPLAINTS": "COMPLAINT",
            "CONCERNS": "CONCERN",
            "NULL": "NEUTRAL",
            "N/A": "NEUTRAL",
            "": "NEUTRAL"
        })
    )

def prepare_labels(df: pd.DataFrame, label_col: str = COL_LABEL) -> np.ndarray:
    y = (
        df[label_col]
        .fillna("NEUTRAL")
        .astype(str)
        .str.upper()
        .str.strip()
        .replace({
            "COMPLAINTS": "COMPLAINT",
            "CONCERNS": "CONCERN",
            "NULL": "NEUTRAL",
            "N/A": "NEUTRAL",
            "": "NEUTRAL"
        })
    )
    return y.map(LABEL2ID).values

# =========================
# Keyword discovery
# =========================
from sklearn.feature_extraction.text import TfidfVectorizer

def build_candidate_phrases(texts: List[str]) -> List[str]:
    tfidf = TfidfVectorizer(
        max_features=TFIDF_MAX_FEAT,
        ngram_range=TFIDF_NGRAM,
        token_pattern=r"(?u)\b\w+\b",
        lowercase=True,
        stop_words="english",
        min_df=3
    )
    X = tfidf.fit_transform(texts)
    vocab = np.array(tfidf.get_feature_names_out())
    scores = np.asarray(X.sum(axis=0)).ravel()
    idx = np.argsort(scores)[::-1]
    top = vocab[idx][:TOP_K_CANDIDATES]
    cleaned = []
    for p in top:
        if len(p) < 3:
            continue
        tokens = [w for w in p.split() if w not in ENGLISH_STOP_WORDS]
        if not tokens:
            continue
        cleaned.append(" ".join(tokens))
    return cleaned[:TOP_K_CANDIDATES]

def expand_keywords_by_similarity(
    model: SentenceTransformer,
    candidates: List[str],
    seed_phrases: List[str],
    top_k: int = TOP_K_PER_CLASS
) -> Tuple[List[str], np.ndarray]:
    if not candidates:
        return [], np.zeros((0, model.get_sentence_embedding_dimension()), dtype=np.float32)
    seed_embs = model.encode(seed_phrases, convert_to_numpy=True, normalize_embeddings=True, batch_size=64)
    centroid = seed_embs.mean(axis=0)
    cand_embs = model.encode(candidates, convert_to_numpy=True, normalize_embeddings=True, batch_size=256)
    sims = cand_embs @ centroid
    top_idx = np.argsort(sims)[::-1][:top_k]
    return [candidates[i] for i in top_idx], cand_embs[top_idx]

# =========================
# Embeddings & Features
# =========================
def embed_transcript(model: SentenceTransformer, text: str) -> np.ndarray:
    chunks = chunk_text(text)
    if not chunks:
        return np.zeros(model.get_sentence_embedding_dimension(), dtype=np.float32)
    embs = model.encode(chunks, convert_to_numpy=True, normalize_embeddings=True, batch_size=32)
    return embs.mean(axis=0).astype(np.float32)

def build_feature_matrix(
    model: SentenceTransformer,
    df: pd.DataFrame,
    comp_kw: List[str],
    comp_kw_emb: np.ndarray,
    con_kw: List[str],
    con_kw_emb: np.ndarray,
    scaler: Optional[MinMaxScaler] = None,
    fit_scaler: bool = False
) -> Tuple[np.ndarray, np.ndarray, MinMaxScaler]:
    texts = text_series(df).tolist()
    N = len(texts)

    # Embeddings
    emb_list = [embed_transcript(model, t) for t in texts]
    E = np.vstack(emb_list)  # (N, d)

    # Keyword counts (simple substring hits; lower-cased)
    def count_hits(s: str, kws: List[str]) -> int:
        s_low = s.lower()
        return sum(1 for k in kws if k in s_low)

    comp_counts = np.array([count_hits(t, comp_kw) for t in texts], dtype=np.float32)
    con_counts  = np.array([count_hits(t, con_kw)  for t in texts], dtype=np.float32)

    # Semantic similarity to keyword centroids
    def centroid(embs: np.ndarray) -> np.ndarray:
        if embs.size == 0:
            return np.zeros(E.shape[1], dtype=np.float32)
        return embs.mean(axis=0).astype(np.float32)

    comp_centroid = centroid(comp_kw_emb)
    con_centroid  = centroid(con_kw_emb)

    def safe_cosine_to_centroid(mat: np.ndarray, c: np.ndarray) -> np.ndarray:
        if np.allclose(c, 0.0):
            return np.zeros((mat.shape[0],), dtype=np.float32)
        denom = (np.linalg.norm(mat, axis=1) * (np.linalg.norm(c) + 1e-12) + 1e-12)
        sims = (mat @ c) / denom
        return np.nan_to_num(sims, nan=0.0).astype(np.float32)

    sims_comp = safe_cosine_to_centroid(E, comp_centroid)
    sims_con  = safe_cosine_to_centroid(E, con_centroid)

    # Numeric inputs
    loud = df.get(COL_LOUDNESS, pd.Series([0]*N)).astype(float).values.astype(np.float32)
    sent = df.get(COL_SENTIMENT, pd.Series([0]*N)).astype(float).values.astype(np.float32)

    # Rule flags
    rule_complaint = ((loud > LOUD_HIGH_TH) & (sent <= SENT_STRONG_NEG) &
                      ((comp_counts > 0) | (sims_comp >= SIM_THRESHOLD))).astype(np.float32)
    rule_concern   = ((sent > SENT_STRONG_NEG) & (sent <= SENT_MOD_NEG_MAX) &
                      ((con_counts > 0) | (sims_con >= SIM_THRESHOLD))).astype(np.float32)
    rule_neutral   = ((sent >= NEU_POS_TH) &
                      (comp_counts + con_counts == 0) &
                      (sims_comp < SIM_THRESHOLD) &
                      (sims_con  < SIM_THRESHOLD)).astype(np.float32)

    # Scale numeric block
    numeric = np.vstack([loud, sent, comp_counts, con_counts, sims_comp, sims_con]).T  # (N, 6)
    if scaler is None:
        scaler = MinMaxScaler()
        fit_scaler = True
    if fit_scaler:
        scaler.fit(numeric)
    numeric_scaled = scaler.transform(numeric).astype(np.float32)

    # Fuse
    X = np.hstack([
        E,
        numeric_scaled,
        rule_complaint.reshape(-1, 1),
        rule_concern.reshape(-1, 1),
        rule_neutral.reshape(-1, 1)
    ]).astype(np.float32)

    meta_cols = np.array([
        "loudness_scaled", "sentiment_scaled",
        "complaint_kw_count_scaled", "concern_kw_count_scaled",
        "sim_comp_scaled", "sim_con_scaled",
        "rule_complaint", "rule_concern", "rule_neutral"
    ])
    return X, meta_cols, scaler

def build_rule_onehot(rule_neu: np.ndarray, rule_con: np.ndarray, rule_comp: np.ndarray) -> np.ndarray:
    return np.vstack([rule_neu, rule_con, rule_comp]).T.astype(np.float32)

def blended_predict_proba(clf: LogisticRegression, X: np.ndarray, rule_flags: np.ndarray, alpha: float = ALPHA_RULE_BLEND) -> np.ndarray:
    proba = clf.predict_proba(X)
    rf = rule_flags.astype(np.float32)
    blended = proba + alpha * rf
    blended = blended / blended.sum(axis=1, keepdims=True)
    return blended

# =========================
# Training / Evaluation
# =========================
def main():
    # Load table (CSV or Excel)
    df = read_table(INPUT_FILE, sheet=SHEET_NAME)

    # Basic checks
    for col in (COL_LABEL, COL_TEXT_CUST, COL_SENTIMENT, COL_LOUDNESS):
        if col not in df.columns:
            raise KeyError(f"Missing required column: {col}")

    # Normalize labels, filter valid
    normalize_labels_inplace(df, COL_LABEL)
    df = df[df[COL_LABEL].isin(LABEL2ID.keys())].reset_index(drop=True)

    # Stratified split 70/15/15
    train_df, test_df = train_test_split(
        df, test_size=0.15, random_state=RANDOM_SEED, stratify=df[COL_LABEL]
    )
    train_df, val_df  = train_test_split(
        train_df, test_size=0.1765, random_state=RANDOM_SEED, stratify=train_df[COL_LABEL]
    )

    # Show label distribution
    for name, d in [("train", train_df), ("val", val_df), ("test", test_df)]:
        print(name, d[COL_LABEL].value_counts().to_dict())

    # Embedding model
    model = SentenceTransformer(EMBED_MODEL)

    # --------------------------
    # Data-driven keywords from TRAIN (seed lists include your attachments)
    # --------------------------
    train_texts = text_series(train_df).tolist()

    # Complaint seeds (expanded with screenshot tags & synonyms)
    complaint_seeds = [
        # screenshot tags (probable complaints)
        "financial difficulties", "vulnerable consent statement", "disconnected callers",
        "high risk vulnerable mental health", "everyday saver", "test ctv", "bridging question vulnerable",
        "complaints existing", "refund", "refer from branch", "chat to voice",
        "rt disssol sentiment test", "high risk vulnerable mental health rt", "complaints new",
        # domain synonyms and obvious complaint phrases
        "complaint", "complaints", "i want a refund", "refund request", "chargeback", "dispute",
        "disputed transaction", "billing error", "overcharged", "unauthorised transaction", "unauthorized",
        "want to cancel", "cancel my account", "escalate", "speak to manager", "escalation",
        "poor service", "bad experience", "not satisfied", "unhappy", "file a complaint",
        "refer from branch", "complaint existing", "complaint new", "complaint about", "refund please",
        "card replacement", "lost and stolen", "lost card", "stolen card", "replace my card",
        "fraud", "scam", "unauthorised", "unauthorised charge", "chargeback", "claim refund"
    ]

    # Concern seeds (expanded with screenshot tags & synonyms)
    concern_seeds = [
        # screenshot tags (probable concerns)
        "card replacement", "pinsentry", "isa", "digital banking registration", "fraud scam",
        "continuous authority", "response to contact letter", "lost and stolen enquiry",
        "scripting", "interest rate query", "potential call disconnect", "disputed transactions",
        "customer confusion", "digital error code", "pins entry", "pin sentry",
        # domain synonyms and common concern phrases
        "concern", "worry", "doubt", "not sure", "need clarification", "query", "question",
        "unsure", "need help", "confused", "information", "clarify", "how to", "how do i",
        "interest rate", "rate query", "payment failed", "transaction failed", "billing question",
        "digital banking", "login issue", "can't login", "forgot password", "scripting issue",
        "call dropped", "potential disconnect", "lost and stolen enquiry", "dispute question"
    ]

    # Build candidate phrases and expand using embeddings
    candidates = build_candidate_phrases(train_texts)
    comp_kw, comp_kw_emb = expand_keywords_by_similarity(model, candidates, complaint_seeds, TOP_K_PER_CLASS)
    con_kw,  con_kw_emb  = expand_keywords_by_similarity(model, candidates, concern_seeds,   TOP_K_PER_CLASS)

    # Persist keyword lists
    with open(os.path.join(OUTPUT_DIR, "complaint_keywords.json"), "w") as f:
        json.dump(comp_kw, f, indent=2)
    with open(os.path.join(OUTPUT_DIR, "concern_keywords.json"), "w") as f:
        json.dump(con_kw, f, indent=2)

    # --------------------------
    # Features
    # --------------------------
    X_train, meta_cols, scaler = build_feature_matrix(model, train_df, comp_kw, comp_kw_emb, con_kw, con_kw_emb, scaler=None, fit_scaler=True)
    X_val,   _,        _      = build_feature_matrix(model, val_df,   comp_kw, comp_kw_emb, con_kw, con_kw_emb, scaler=scaler, fit_scaler=False)
    X_test,  _,        _      = build_feature_matrix(model, test_df,  comp_kw, comp_kw_emb, con_kw, con_kw_emb, scaler=scaler, fit_scaler=False)

    y_train = prepare_labels(train_df)
    y_val   = prepare_labels(val_df)
    y_test  = prepare_labels(test_df)

    # Rule flags from fused X (last 3 columns)
    def split_rule_flags(X: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
        rule_comp = X[:, -3]
        rule_con  = X[:, -2]
        rule_neu  = X[:, -1]
        return rule_neu, rule_con, rule_comp

    rneu_va, rcon_va, rcomp_va = split_rule_flags(X_val)
    rneu_te, rcon_te, rcomp_te = split_rule_flags(X_test)

    # --------------------------
    # Train classifier
    # --------------------------
    clf = LogisticRegression(
        max_iter=1500,
        class_weight="balanced",
        multi_class="auto",
        solver="lbfgs",
        random_state=RANDOM_SEED
    )
    clf.fit(X_train, y_train)

    # Pure ML
    print("\n=== Pure ML (no blending) ===")
    y_pred_val = clf.predict(X_val)
    print("Validation:")
    print(classification_report(
        y_val, y_pred_val,
        labels=[0,1,2],
        target_names=["NEUTRAL","CONCERN","COMPLAINT"],
        zero_division=0
    ))
    y_pred_tst = clf.predict(X_test)
    print("Test:")
    print(classification_report(
        y_test, y_pred_tst,
        labels=[0,1,2],
        target_names=["NEUTRAL","CONCERN","COMPLAINT"],
        zero_division=0
    ))

    # Hybrid (blend rules + ML)
    print("\n=== Hybrid (ML + rule blending) ===")
    rule_onehot_val = build_rule_onehot(rneu_va, rcon_va, rcomp_va)
    rule_onehot_tst = build_rule_onehot(rneu_te, rcon_te, rcomp_te)

    proba_val_blend = blended_predict_proba(clf, X_val, rule_onehot_val, alpha=ALPHA_RULE_BLEND)
    proba_tst_blend = blended_predict_proba(clf, X_test, rule_onehot_tst, alpha=ALPHA_RULE_BLEND)

    y_pred_val_blend = np.argmax(proba_val_blend, axis=1)
    y_pred_tst_blend = np.argmax(proba_tst_blend, axis=1)

    print("Validation:")
    print(classification_report(
        y_val, y_pred_val_blend,
        labels=[0,1,2],
        target_names=["NEUTRAL","CONCERN","COMPLAINT"],
        zero_division=0
    ))
    print("Test:")
    print(classification_report(
        y_test, y_pred_tst_blend,
        labels=[0,1,2],
        target_names=["NEUTRAL","CONCERN","COMPLAINT"],
        zero_division=0
    ))

    # Save artifacts
    joblib.dump(clf, os.path.join(OUTPUT_DIR, "hybrid_logreg.joblib"))
    joblib.dump(scaler, os.path.join(OUTPUT_DIR, "numeric_scaler.joblib"))
    with open(os.path.join(OUTPUT_DIR, "meta_cols.json"), "w") as f:
        json.dump({"meta_cols": meta_cols.tolist()}, f)
    with open(os.path.join(OUTPUT_DIR, "embed_model.txt"), "w") as f:
        f.write(EMBED_MODEL)
    with open(os.path.join(OUTPUT_DIR, "label_mapping.json"), "w") as f:
        json.dump({"LABEL2ID": LABEL2ID, "ID2LABEL": ID2LABEL}, f, indent=2)

    print(f"\nArtifacts saved to: {OUTPUT_DIR}")

# =========================
# Inference (batch)
# =========================
def inference(input_path: str,
              artifacts_dir: str = OUTPUT_DIR,
              output_path: str = "predictions_output.xlsx",  # write Excel by default
              blend_with_rules: bool = True,
              alpha: float = ALPHA_RULE_BLEND,
              sheet: Optional[str] = None) -> pd.DataFrame:
    df_new = read_table(input_path, sheet=sheet)
    # labels may be absent; normalize anyway for consistency
    if COL_LABEL in df_new.columns:
        normalize_labels_inplace(df_new, COL_LABEL)

    # Load artifacts
    clf = joblib.load(os.path.join(artifacts_dir, "hybrid_logreg.joblib"))
    scaler = joblib.load(os.path.join(artifacts_dir, "numeric_scaler.joblib"))
    with open(os.path.join(artifacts_dir, "embed_model.txt"), "r") as f:
        emb_name = f.read().strip()
    model = SentenceTransformer(emb_name)

    with open(os.path.join(artifacts_dir, "complaint_keywords.json"), "r") as f:
        comp_kw = json.load(f)
    with open(os.path.join(artifacts_dir, "concern_keywords.json"), "r") as f:
        con_kw = json.load(f)

    # Recompute keyword centroid embeddings
    cand = comp_kw + con_kw
    if cand:
        all_embs = model.encode(cand, convert_to_numpy=True, normalize_embeddings=True, batch_size=256)
        comp_kw_emb = all_embs[:len(comp_kw)]
        con_kw_emb  = all_embs[len(comp_kw):]
    else:
        comp_kw_emb = np.zeros((0, model.get_sentence_embedding_dimension()), dtype=np.float32)
        con_kw_emb  = np.zeros((0, model.get_sentence_embedding_dimension()), dtype=np.float32)

    # Features
    X_new, _, _ = build_feature_matrix(model, df_new, comp_kw, comp_kw_emb, con_kw, con_kw_emb, scaler=scaler, fit_scaler=False)

    # Predict
    proba_ml = clf.predict_proba(X_new)
    if blend_with_rules:
        rule_comp = X_new[:, -3]
        rule_con  = X_new[:, -2]
        rule_neu  = X_new[:, -1]
        rule_onehot = build_rule_onehot(rule_neu, rule_con, rule_comp)
        proba = blended_predict_proba(clf, X_new, rule_onehot, alpha=alpha)
    else:
        proba = proba_ml

    y_hat = np.argmax(proba, axis=1)
    preds = [ID2LABEL[i] for i in y_hat]

    # Output
    df_new["pred_class"]        = preds
    df_new["proba_NEUTRAL"]     = proba[:, 0]
    df_new["proba_CONCERN"]     = proba[:, 1]
    df_new["proba_COMPLAINT"]   = proba[:, 2]

    write_table(df_new, output_path)
    print(f"Saved predictions to: {output_path}")
    return df_new

# =========================
# Entrypoint
# =========================
if __name__ == "__main__":
    main()
    # Example inference calls:
    # inference("unseen_calls.xlsx", artifacts_dir=OUTPUT_DIR, output_path="unseen_predictions.xlsx", sheet="Sheet1")
    # inference("unseen_calls.csv", artifacts_dir=OUTPUT_DIR, output_path="unseen_predictions.csv")