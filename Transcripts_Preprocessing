# =========================
# STEP 2: Preprocessing pipeline for transcripts
# =========================
import re
import unicodedata
import pandas as pd
import spacy

# Load spaCy once
nlp = spacy.load("en_core_web_sm", disable=["parser", "ner"])

# ---------- Config ----------
SEP_TOKEN = "[SEP]"   # Hard token, not lemmatized

# Filler words/phrases
FILLER_WORDS = {
    "um","uh","umm","uhh","erm","er","eh","hmm","mm","mmm","ah","oh","uhm","ya","yeahh",
    "umhum","mhm","huh","ummm","hmmm"
}
FILLER_PHRASES = [
    r"\byou know\b",
    r"\bi mean\b",
    r"\bkind of\b",
    r"\bsort of\b",
    r"\bto be honest\b",
    r"\bto be fair\b",
    r"\bif that makes sense\b"
]

# Negations to preserve
NEGATIONS = {"no", "not", "never", "n't"}


# ---------- Helpers ----------
def normalize_unicode(text: str) -> str:
    return unicodedata.normalize("NFKC", text)

def scrub_bracketed(text: str) -> str:
    return re.sub(r"\[[^\]]+\]", " ", text)   # remove [PII], [NAME], etc.

def insert_speaker_delims(text: str) -> str:
    t = re.sub(r"[\r\n]+", " ", text)
    return re.sub(r"\b(AGENT|CUSTOMER)\b", r"||| \1", t)

def split_by_speaker(text: str):
    chunks = [c.strip() for c in insert_speaker_delims(text).split("|||") if c.strip()]
    segments, current = [], None
    for ch in chunks:
        if ch.startswith("AGENT") or ch.startswith("CUSTOMER"):
            spk = "AGENT" if ch.startswith("AGENT") else "CUSTOMER"
            content = ch[len(spk):].strip(" :-\t")
            if content:
                segments.append((spk, content))
            current = spk
        else:
            if current:
                segments.append((current, ch))
            else:
                segments.append(("UNK", ch))
    return segments

def collapse_stutters(text: str) -> str:
    text = re.sub(r"\b(\w+)\s*[-–—]{1,2}\s*\1\b", r"\1", text)
    text = re.sub(r"\b(\w+)\s+\1\b", r"\1", text)
    text = re.sub(r"(.)\1{2,}", r"\1\1", text)  # soooo -> soo
    return text

def remove_fillers(text: str) -> str:
    for pat in FILLER_PHRASES:
        text = re.sub(pat, " ", text, flags=re.IGNORECASE)
    if FILLER_WORDS:
        pat = r"\b(" + "|".join(re.escape(w) for w in sorted(FILLER_WORDS, key=len, reverse=True)) + r")\b"
        text = re.sub(pat, " ", text, flags=re.IGNORECASE)
    return text

def basic_normalize(text: str) -> str:
    text = text.lower()
    text = re.sub(r"[^\w\s\.\!\?\'/:-]", " ", text)
    text = re.sub(r"\s+", " ", text).strip()
    return text

def tag_money(text: str) -> str:
    text = re.sub(r"(\£|\$|\€)\s*\d+([.,]\d+)?", " [MONEY] ", text)
    text = re.sub(r"\d+\s*(rs|inr|usd|eur|gbp)", " [MONEY] ", text, flags=re.IGNORECASE)
    text = re.sub(r"\d+(\.\d+)?\s*%", " [MONEY] ", text)
    return text

def lemmatize_keep_negations(text: str) -> str:
    doc = nlp(text)
    out = []
    for tok in doc:
        if tok.is_space:
            continue
        if tok.text == SEP_TOKEN:  # preserve separator
            out.append(SEP_TOKEN)
            continue
        lemma = tok.lemma_.strip()
        if not lemma:
            continue
        if (not tok.is_stop) or (lemma in NEGATIONS or tok.lower_ in NEGATIONS):
            out.append(lemma)
    return " ".join(out)

def assemble_text(segments, customer_only, keep_turns, keep_speakers=False):
    utts = []
    for spk, u in segments:
        if customer_only and spk != "CUSTOMER":
            continue
        if keep_speakers:
            utts.append(f"{spk}: {u}")
        else:
            utts.append(u)
    return f" {SEP_TOKEN} ".join(utts) if keep_turns else " ".join(utts)


# ---------- Main pipeline ----------
def preprocess_transcript(raw_text: str,
                          mode: str = "light",
                          customer_only: bool = False,
                          keep_turns: bool = True,
                          keep_speakers: bool = False) -> str:
    """
    mode = "light" -> minimal cleanup (readable, for embeddings)
    mode = "strict" -> aggressive lemmatization + stopword removal
    """
    if not isinstance(raw_text, str) or not raw_text.strip():
        return ""

    t = normalize_unicode(raw_text)
    t = scrub_bracketed(t)
    t = collapse_stutters(t)
    segments = split_by_speaker(t)
    t = assemble_text(segments, customer_only=customer_only, keep_turns=keep_turns, keep_speakers=keep_speakers)

    t = remove_fillers(t)
    t = basic_normalize(t)
    t = tag_money(t)

    if mode == "strict":
        t = lemmatize_keep_negations(t)

    t = re.sub(r"\s+", " ", t).strip()
    return t


# ---------- DataFrame Wrapper ----------
def preprocess_dataframe(df: pd.DataFrame,
                         transcript_col_full: str = "transcripts",
                         transcript_col_cust: str = "Customer transcript") -> pd.DataFrame:
    """
    Adds four new columns:
      - full_light, full_strict
      - customer_light, customer_strict
    """
    texts_full = df[transcript_col_full].fillna("").astype(str).tolist()
    texts_cust = df[transcript_col_cust].fillna("").astype(str).tolist()

    out = df.copy()
    out["full_light"] = [preprocess_transcript(t, mode="light", customer_only=False, keep_turns=True, keep_speakers=False) for t in texts_full]
    out["full_strict"] = [preprocess_transcript(t, mode="strict", customer_only=False, keep_turns=True, keep_speakers=False) for t in texts_full]
    out["customer_light"] = [preprocess_transcript(t, mode="light", customer_only=True, keep_turns=True, keep_speakers=False) for t in texts_cust]
    out["customer_strict"] = [preprocess_transcript(t, mode="strict", customer_only=True, keep_turns=True, keep_speakers=False) for t in texts_cust]
    return out
